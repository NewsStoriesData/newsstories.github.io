<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <title>NewsStories: Illustrating articles with visual summaries</title>
    <link media="all" href="css.css" type="text/css" rel="StyleSheet">

  </head>
  <body>
<body>
<div id="primarycontent">
<center><h1><b>NewsStories</b>: </br> Illustrating articles with visual summaries</h1></center>
<center><h2>
	<a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a>&nbsp;&nbsp;&nbsp;
	<a href="https://bryanplummer.com//">Bryan Plummer</a>&nbsp;&nbsp;&nbsp;
	<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>&nbsp;&nbsp;&nbsp;
	<a href="http://www.scribblethink.org/">JP Lewis</a>
	<a href="https://scholar.google.com/citations?user=zc6Iy0IAAAAJ&hl=en">Avneesh Sud</a>
	<a href="https://scholar.google.com/citations?user=sUK_w2QAAAAJ&hl=en">Thomas Leung</a>
	</h2>

	<center><h2>
		Boston University&nbsp;&nbsp;&nbsp;
		MIT-IBM Watson AI Lab&nbsp;&nbsp;&nbsp;
		Google Research&nbsp;&nbsp;&nbsp;
	</h2></center>
<center><h2>In ECCV 2022</h2></center>
<center><h2><strong><a href="">Paper</a> | <a href="">Code</a> | </strong> </h2></center>

<center style="margin-top:1cm;">
    <img src="motivational_figure.png" width="500">
    </br>
</center>
</br>

</br>
<h2 align="center">Abstract</h2>
</br>
<div style="font-size:14px"><p align="justify"> Recent self-supervised approaches have used large-scale image-text datasets to learn powerful representations that transfer to many tasks without finetuning. These methods often assume that there is one-to-one correspondence between its images and their short captions. However, many tasks require reasoning about multiple images and long text narratives, such as describing news articles with visual summaries. Thus, we explore a novel setting where the goal is to learn a self-supervised visual-language representation that is robust to varying text length and the number of images. In addition, unlike prior work which assumed captions have a literal relation to the image, we assume images only contain loose illustrative correspondence with the text.  To explore this problem, we introduce a large-scale multimodal dataset containing over 900,000 articles grouped into 350,000 stories and more than 750,000 images. We  show that state-of-the-art image-text alignment methods are not robust to longer narratives with multiple images. 
Finally, we introduce an intuitive baseline that outperforms these methods on zero-shot image-set retrieval by 10% on the GoodNews dataset. </p></div>

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vS912hgf0ON8uWtjhhGmsvs2lHnG6bmG2Kf28MleUFlnyVSIeV_5m1D4RdB7HbXOieWiy8ewZ2B-xTD/embed?start=false&loop=false&delayms=60000" frameborder="0" width="800" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<a href="https://docs.google.com/presentation/d/1_rV2pnMOoEJ1kXzunJ-ARbLuY8QKdw050nS0lHXIATY/edit?usp=sharing">[slide deck link]</a>

</br>
</br>
</br>
<h2 align="center">Citation</h2>
<pre align="left"><code align="left">
@inproceedings{rxtan2022newsstories,
    author    = {Tan, Reuben and Plummer, Bryan and Saenko, Kate and Lewis, JP and Sud, Avneesh and Leung, Thomas},
    title     = {NewsStories: Illustrating articles with visual summaries},
    booktitle = {Proceedings of the IEEE/CVF European Conference on Computer Vision (ECCV)},
    month     = {October},
    year      = {2022}
}
</pre></code>
</br></br></br></br>
</body>
