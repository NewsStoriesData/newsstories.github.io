<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <title>MetaPose: NewsStories: Illustrating articles with visual summaries</title>
    <link media="all" href="css.css" type="text/css" rel="StyleSheet">

  </head>
  <body>
<body>
<div id="primarycontent">
<center><h1><b>MetaPose</b>: </br> Fast 3D Pose from Multiple Views without 3D Supervision</h1></center>
<center><h2>
	<a href="https://cs-people.bu.edu/rxtan/">Reuben Tan</a>&nbsp;&nbsp;&nbsp;
	<a href="https://bryanplummer.com//">Bryan Plummer</a>&nbsp;&nbsp;&nbsp;
	<a href="http://ai.bu.edu/ksaenko.html">Kate Saenko</a>&nbsp;&nbsp;&nbsp;
	<a href="http://www.scribblethink.org/">JP Lewis</a>
	<a href="https://scholar.google.com/citations?user=zc6Iy0IAAAAJ&hl=en">Avneesh Sud</a>
	<a href="https://scholar.google.com/citations?user=sUK_w2QAAAAJ&hl=en">Thomas Leung</a>
	</h2>

	<center><h2>
		Boston University&nbsp;&nbsp;&nbsp;
		MIT-IBM Watson AI Lab&nbsp;&nbsp;&nbsp;
		Google Research&nbsp;&nbsp;&nbsp;
	</h2></center>
<center><h2>In ECCV 2022</h2></center>
<center><h2><strong><a href="">Paper</a> | <a href="">Code</a> | </strong> </h2></center>

<center style="margin-top:1cm;">
    <img src="task.png" width="500">
    </br>
    </br>
    <img src="advantages.png" width="500">
</center>
</br>
<div style="font-size:14px"><p align="justify">
MetaPose accurately estimates <b>3D human poses</b>, takes into account <b>multi-view uncertainty</b>, and uses only <b>2D supervision</b> for training!
It is <b>faster</b> and <b>more accurate</b>, especially with fewer cameras.
</p></div>

</br>
<h2 align="center">Abstract</h2>
</br>
<div style="font-size:14px"><p align="justify"> Recent self-supervised approaches have used large-scale image-text datasets to learn powerful representations that transfer to many tasks without finetuning. These methods often assume that there is one-to-one correspondence between its images and their short captions. However, many tasks require reasoning about multiple images and long text narratives, such as describing news articles with visual summaries. Thus, we explore a novel setting where the goal is to learn a self-supervised visual-language representation that is robust to varying text length and the number of images. In addition, unlike prior work which assumed captions have a literal relation to the image, we assume images only contain loose illustrative correspondence with the text.  To explore this problem, we introduce a large-scale multimodal dataset containing over 900,000 articles grouped into 350,000 stories and more than 750,000 images. We  show that state-of-the-art image-text alignment methods are not robust to longer narratives with multiple images. 
Finally, we introduce an intuitive baseline that outperforms these methods on zero-shot image-set retrieval by 10% on the GoodNews dataset. </p></div>

<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vS912hgf0ON8uWtjhhGmsvs2lHnG6bmG2Kf28MleUFlnyVSIeV_5m1D4RdB7HbXOieWiy8ewZ2B-xTD/embed?start=false&loop=false&delayms=60000" frameborder="0" width="800" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
<a href="https://docs.google.com/presentation/d/1_rV2pnMOoEJ1kXzunJ-ARbLuY8QKdw050nS0lHXIATY/edit?usp=sharing">[slide deck link]</a>

</br>
</br>
</br>
<h2 align="center">Citation</h2>
<pre align="left"><code align="left">
@inproceedings{usman2021metapose,
    author    = {Usman, Ben and Tagliasacchi, Andrea and Saenko, Kate and Sud, Avneesh},
    title     = {MetaPose: Fast 3D Pose from Multiple Views without 3D Supervision},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022}
}
</pre></code>
</br></br></br></br>
</body>
